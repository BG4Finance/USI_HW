{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML1 Bardi Simon Siam Pucar.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BG4Finance/USI_HW/blob/master/MachineLearning_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3cz0hfUMleB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    \"\"\"\n",
        "    Loads the data from a saved .npz file.\n",
        "    ### YOU CAN NOT EDIT THIS FUNCTION ###\n",
        "    :param filename: string, path to the .npz file storing the data.\n",
        "    :return: two numpy arrays:\n",
        "        - x, a Numpy array of shape (n_samples, n_features) with the inputs;\n",
        "        - y, a Numpy array of shape (n_samples, ) with the targets.\n",
        "    \"\"\"\n",
        "    data = np.load(filename)\n",
        "    x = data['x']\n",
        "    y = data['y']\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def evaluate_predictions(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Evaluates the mean squared error between the values in y_true and the values\n",
        "    in y_pred.\n",
        "    ### YOU CAN NOT EDIT THIS FUNCTION ###\n",
        "    :param y_true: Numpy array, the true target values from the test set;\n",
        "    :param y_pred: Numpy array, the values predicted by your model.\n",
        "    :return: float, the the mean squared error between the two arrays.\n",
        "    \"\"\"\n",
        "    return ((y_true - y_pred) ** 2).mean()\n",
        "\n",
        "\n",
        "def load_model(filename):\n",
        "    \"\"\"\n",
        "    Loads a Scikit-learn model saved with joblib.dump.\n",
        "    This is just an example, you can write your own function to load the model.\n",
        "    Some examples can be found in src/utils.py.\n",
        "    :param filename: string, path to the file storing the model.\n",
        "    :return: the model.\n",
        "    \"\"\"\n",
        "    model = joblib.load(filename)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao-b7rmKTWZs",
        "colab_type": "code",
        "outputId": "1fdf6694-62a8-4fd2-861f-bee797cdd7e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "x,y = load_data(\"data.npz\")\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1200, 2)\n",
            "(1200,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZTxEcQ0OQRP",
        "colab_type": "code",
        "outputId": "cdacf13b-4e3c-4f44-9539-8c335c7190d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Perform linear regression\n",
        "XX = np.linalg.inv(x.T @ x)\n",
        "EsT= XX @ x.T @ y\n",
        "print(EsT)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.00506449 -0.01796875]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKYCwBskPnae",
        "colab_type": "code",
        "outputId": "2f37fa1b-e74b-4a80-cfd6-0063953c39fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8262
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras import utils, models\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation\n",
        "from keras.optimizers import adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "STOP = EarlyStopping(monitor='val_loss', mode='min', patience=50) \n",
        "y    = np.reshape(y, (-1,1))\n",
        "x, x_test, y, y_test = train_test_split(x, y, test_size=0.15)   \n",
        "\n",
        "#model creation\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(2000,activation=\"relu\"))\n",
        "model.add(Dense(500,activation=\"relu\"))\n",
        "model.add(Dense(100,activation=\"relu\"))\n",
        "model.add(Dense(10,activation=\"relu\"))\n",
        "model.add(Dense(1, activation='elu'))\n",
        "\n",
        "#compile as Classification problem\n",
        "model.compile(optimizer=adam(lr=0.0005), loss='mse')\n",
        "#fit\n",
        "storia = model.fit(x, y, batch_size=64, epochs=2000, validation_split=0.2,callbacks=[STOP])\n",
        "#save\n",
        "models.save_model(model, \"nlmodel\") "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 816 samples, validate on 204 samples\n",
            "Epoch 1/2000\n",
            "816/816 [==============================] - 1s 1ms/step - loss: 0.3454 - val_loss: 0.2946\n",
            "Epoch 2/2000\n",
            "816/816 [==============================] - 0s 488us/step - loss: 0.3242 - val_loss: 0.2898\n",
            "Epoch 3/2000\n",
            "816/816 [==============================] - 0s 485us/step - loss: 0.3238 - val_loss: 0.2813\n",
            "Epoch 4/2000\n",
            "816/816 [==============================] - 0s 486us/step - loss: 0.3042 - val_loss: 0.2509\n",
            "Epoch 5/2000\n",
            "816/816 [==============================] - 0s 518us/step - loss: 0.2819 - val_loss: 0.2340\n",
            "Epoch 6/2000\n",
            "816/816 [==============================] - 0s 569us/step - loss: 0.2581 - val_loss: 0.2045\n",
            "Epoch 7/2000\n",
            "816/816 [==============================] - 0s 606us/step - loss: 0.2122 - val_loss: 0.1588\n",
            "Epoch 8/2000\n",
            "816/816 [==============================] - 0s 560us/step - loss: 0.1718 - val_loss: 0.1440\n",
            "Epoch 9/2000\n",
            "816/816 [==============================] - 0s 592us/step - loss: 0.1514 - val_loss: 0.1254\n",
            "Epoch 10/2000\n",
            "816/816 [==============================] - 0s 570us/step - loss: 0.1366 - val_loss: 0.1248\n",
            "Epoch 11/2000\n",
            "816/816 [==============================] - 0s 565us/step - loss: 0.1251 - val_loss: 0.0960\n",
            "Epoch 12/2000\n",
            "816/816 [==============================] - 0s 543us/step - loss: 0.1099 - val_loss: 0.0920\n",
            "Epoch 13/2000\n",
            "816/816 [==============================] - 0s 550us/step - loss: 0.1052 - val_loss: 0.0840\n",
            "Epoch 14/2000\n",
            "816/816 [==============================] - 0s 565us/step - loss: 0.0940 - val_loss: 0.0817\n",
            "Epoch 15/2000\n",
            "816/816 [==============================] - 0s 561us/step - loss: 0.0890 - val_loss: 0.0799\n",
            "Epoch 16/2000\n",
            "816/816 [==============================] - 0s 569us/step - loss: 0.0829 - val_loss: 0.0713\n",
            "Epoch 17/2000\n",
            "816/816 [==============================] - 0s 553us/step - loss: 0.0765 - val_loss: 0.0693\n",
            "Epoch 18/2000\n",
            "816/816 [==============================] - 0s 552us/step - loss: 0.0756 - val_loss: 0.0747\n",
            "Epoch 19/2000\n",
            "816/816 [==============================] - 0s 595us/step - loss: 0.0729 - val_loss: 0.0661\n",
            "Epoch 20/2000\n",
            "816/816 [==============================] - 0s 585us/step - loss: 0.0689 - val_loss: 0.0677\n",
            "Epoch 21/2000\n",
            "816/816 [==============================] - 0s 578us/step - loss: 0.0666 - val_loss: 0.0649\n",
            "Epoch 22/2000\n",
            "816/816 [==============================] - 0s 567us/step - loss: 0.0645 - val_loss: 0.0595\n",
            "Epoch 23/2000\n",
            "816/816 [==============================] - 0s 563us/step - loss: 0.0617 - val_loss: 0.0607\n",
            "Epoch 24/2000\n",
            "816/816 [==============================] - 0s 549us/step - loss: 0.0613 - val_loss: 0.0600\n",
            "Epoch 25/2000\n",
            "816/816 [==============================] - 0s 539us/step - loss: 0.0601 - val_loss: 0.0591\n",
            "Epoch 26/2000\n",
            "816/816 [==============================] - 0s 552us/step - loss: 0.0600 - val_loss: 0.0623\n",
            "Epoch 27/2000\n",
            "816/816 [==============================] - 0s 543us/step - loss: 0.0584 - val_loss: 0.0583\n",
            "Epoch 28/2000\n",
            "816/816 [==============================] - 0s 559us/step - loss: 0.0595 - val_loss: 0.0522\n",
            "Epoch 29/2000\n",
            "816/816 [==============================] - 0s 574us/step - loss: 0.0582 - val_loss: 0.0589\n",
            "Epoch 30/2000\n",
            "816/816 [==============================] - 0s 558us/step - loss: 0.0576 - val_loss: 0.0553\n",
            "Epoch 31/2000\n",
            "816/816 [==============================] - 0s 542us/step - loss: 0.0566 - val_loss: 0.0467\n",
            "Epoch 32/2000\n",
            "816/816 [==============================] - 0s 588us/step - loss: 0.0529 - val_loss: 0.0501\n",
            "Epoch 33/2000\n",
            "816/816 [==============================] - 0s 558us/step - loss: 0.0514 - val_loss: 0.0481\n",
            "Epoch 34/2000\n",
            "816/816 [==============================] - 0s 537us/step - loss: 0.0497 - val_loss: 0.0491\n",
            "Epoch 35/2000\n",
            "816/816 [==============================] - 0s 545us/step - loss: 0.0494 - val_loss: 0.0474\n",
            "Epoch 36/2000\n",
            "816/816 [==============================] - 0s 540us/step - loss: 0.0466 - val_loss: 0.0423\n",
            "Epoch 37/2000\n",
            "816/816 [==============================] - 0s 556us/step - loss: 0.0492 - val_loss: 0.0431\n",
            "Epoch 38/2000\n",
            "816/816 [==============================] - 0s 537us/step - loss: 0.0470 - val_loss: 0.0460\n",
            "Epoch 39/2000\n",
            "816/816 [==============================] - 0s 538us/step - loss: 0.0472 - val_loss: 0.0407\n",
            "Epoch 40/2000\n",
            "816/816 [==============================] - 0s 559us/step - loss: 0.0443 - val_loss: 0.0422\n",
            "Epoch 41/2000\n",
            "816/816 [==============================] - 0s 546us/step - loss: 0.0455 - val_loss: 0.0417\n",
            "Epoch 42/2000\n",
            "816/816 [==============================] - 0s 522us/step - loss: 0.0455 - val_loss: 0.0500\n",
            "Epoch 43/2000\n",
            "816/816 [==============================] - 0s 521us/step - loss: 0.0471 - val_loss: 0.0477\n",
            "Epoch 44/2000\n",
            "816/816 [==============================] - 0s 514us/step - loss: 0.0457 - val_loss: 0.0395\n",
            "Epoch 45/2000\n",
            "816/816 [==============================] - 0s 512us/step - loss: 0.0451 - val_loss: 0.0400\n",
            "Epoch 46/2000\n",
            "816/816 [==============================] - 0s 522us/step - loss: 0.0416 - val_loss: 0.0367\n",
            "Epoch 47/2000\n",
            "816/816 [==============================] - 0s 520us/step - loss: 0.0415 - val_loss: 0.0396\n",
            "Epoch 48/2000\n",
            "816/816 [==============================] - 0s 518us/step - loss: 0.0454 - val_loss: 0.0392\n",
            "Epoch 49/2000\n",
            "816/816 [==============================] - 0s 525us/step - loss: 0.0498 - val_loss: 0.0397\n",
            "Epoch 50/2000\n",
            "816/816 [==============================] - 0s 512us/step - loss: 0.0457 - val_loss: 0.0372\n",
            "Epoch 51/2000\n",
            "816/816 [==============================] - 0s 522us/step - loss: 0.0413 - val_loss: 0.0373\n",
            "Epoch 52/2000\n",
            "816/816 [==============================] - 0s 548us/step - loss: 0.0402 - val_loss: 0.0361\n",
            "Epoch 53/2000\n",
            "816/816 [==============================] - 0s 569us/step - loss: 0.0416 - val_loss: 0.0386\n",
            "Epoch 54/2000\n",
            "816/816 [==============================] - 0s 558us/step - loss: 0.0457 - val_loss: 0.0363\n",
            "Epoch 55/2000\n",
            "816/816 [==============================] - 0s 547us/step - loss: 0.0440 - val_loss: 0.0407\n",
            "Epoch 56/2000\n",
            "816/816 [==============================] - 0s 577us/step - loss: 0.0437 - val_loss: 0.0474\n",
            "Epoch 57/2000\n",
            "816/816 [==============================] - 0s 536us/step - loss: 0.0410 - val_loss: 0.0385\n",
            "Epoch 58/2000\n",
            "816/816 [==============================] - 0s 560us/step - loss: 0.0421 - val_loss: 0.0358\n",
            "Epoch 59/2000\n",
            "816/816 [==============================] - 0s 592us/step - loss: 0.0418 - val_loss: 0.0339\n",
            "Epoch 60/2000\n",
            "816/816 [==============================] - 0s 572us/step - loss: 0.0385 - val_loss: 0.0350\n",
            "Epoch 61/2000\n",
            "816/816 [==============================] - 0s 541us/step - loss: 0.0370 - val_loss: 0.0319\n",
            "Epoch 62/2000\n",
            "816/816 [==============================] - 0s 539us/step - loss: 0.0384 - val_loss: 0.0362\n",
            "Epoch 63/2000\n",
            "816/816 [==============================] - 0s 519us/step - loss: 0.0404 - val_loss: 0.0327\n",
            "Epoch 64/2000\n",
            "816/816 [==============================] - 0s 534us/step - loss: 0.0390 - val_loss: 0.0341\n",
            "Epoch 65/2000\n",
            "816/816 [==============================] - 0s 539us/step - loss: 0.0383 - val_loss: 0.0341\n",
            "Epoch 66/2000\n",
            "816/816 [==============================] - 0s 541us/step - loss: 0.0368 - val_loss: 0.0350\n",
            "Epoch 67/2000\n",
            "816/816 [==============================] - 0s 540us/step - loss: 0.0389 - val_loss: 0.0335\n",
            "Epoch 68/2000\n",
            "816/816 [==============================] - 0s 522us/step - loss: 0.0480 - val_loss: 0.0669\n",
            "Epoch 69/2000\n",
            "816/816 [==============================] - 0s 536us/step - loss: 0.0503 - val_loss: 0.0478\n",
            "Epoch 70/2000\n",
            "816/816 [==============================] - 0s 535us/step - loss: 0.0454 - val_loss: 0.0420\n",
            "Epoch 71/2000\n",
            "816/816 [==============================] - 0s 553us/step - loss: 0.0423 - val_loss: 0.0355\n",
            "Epoch 72/2000\n",
            "816/816 [==============================] - 0s 536us/step - loss: 0.0386 - val_loss: 0.0364\n",
            "Epoch 73/2000\n",
            "816/816 [==============================] - 0s 541us/step - loss: 0.0364 - val_loss: 0.0323\n",
            "Epoch 74/2000\n",
            "816/816 [==============================] - 0s 531us/step - loss: 0.0381 - val_loss: 0.0323\n",
            "Epoch 75/2000\n",
            "816/816 [==============================] - 0s 541us/step - loss: 0.0388 - val_loss: 0.0315\n",
            "Epoch 76/2000\n",
            "816/816 [==============================] - 0s 575us/step - loss: 0.0381 - val_loss: 0.0386\n",
            "Epoch 77/2000\n",
            "816/816 [==============================] - 0s 551us/step - loss: 0.0421 - val_loss: 0.0479\n",
            "Epoch 78/2000\n",
            "816/816 [==============================] - 0s 547us/step - loss: 0.0435 - val_loss: 0.0341\n",
            "Epoch 79/2000\n",
            "816/816 [==============================] - 0s 587us/step - loss: 0.0364 - val_loss: 0.0311\n",
            "Epoch 80/2000\n",
            "816/816 [==============================] - 0s 560us/step - loss: 0.0388 - val_loss: 0.0348\n",
            "Epoch 81/2000\n",
            "816/816 [==============================] - 0s 541us/step - loss: 0.0367 - val_loss: 0.0297\n",
            "Epoch 82/2000\n",
            "816/816 [==============================] - 0s 546us/step - loss: 0.0363 - val_loss: 0.0336\n",
            "Epoch 83/2000\n",
            "816/816 [==============================] - 0s 528us/step - loss: 0.0389 - val_loss: 0.0329\n",
            "Epoch 84/2000\n",
            "816/816 [==============================] - 0s 539us/step - loss: 0.0356 - val_loss: 0.0307\n",
            "Epoch 85/2000\n",
            "816/816 [==============================] - 0s 555us/step - loss: 0.0362 - val_loss: 0.0318\n",
            "Epoch 86/2000\n",
            "816/816 [==============================] - 0s 542us/step - loss: 0.0351 - val_loss: 0.0331\n",
            "Epoch 87/2000\n",
            "816/816 [==============================] - 0s 544us/step - loss: 0.0384 - val_loss: 0.0297\n",
            "Epoch 88/2000\n",
            "816/816 [==============================] - 0s 532us/step - loss: 0.0365 - val_loss: 0.0323\n",
            "Epoch 89/2000\n",
            "816/816 [==============================] - 0s 536us/step - loss: 0.0370 - val_loss: 0.0305\n",
            "Epoch 90/2000\n",
            "816/816 [==============================] - 0s 531us/step - loss: 0.0336 - val_loss: 0.0310\n",
            "Epoch 91/2000\n",
            "816/816 [==============================] - 0s 535us/step - loss: 0.0337 - val_loss: 0.0290\n",
            "Epoch 92/2000\n",
            "816/816 [==============================] - 0s 534us/step - loss: 0.0329 - val_loss: 0.0293\n",
            "Epoch 93/2000\n",
            "816/816 [==============================] - 0s 534us/step - loss: 0.0348 - val_loss: 0.0396\n",
            "Epoch 94/2000\n",
            "816/816 [==============================] - 0s 546us/step - loss: 0.0397 - val_loss: 0.0308\n",
            "Epoch 95/2000\n",
            "816/816 [==============================] - 0s 534us/step - loss: 0.0384 - val_loss: 0.0323\n",
            "Epoch 96/2000\n",
            "816/816 [==============================] - 0s 550us/step - loss: 0.0363 - val_loss: 0.0293\n",
            "Epoch 97/2000\n",
            "816/816 [==============================] - 0s 539us/step - loss: 0.0351 - val_loss: 0.0296\n",
            "Epoch 98/2000\n",
            "816/816 [==============================] - 0s 542us/step - loss: 0.0330 - val_loss: 0.0275\n",
            "Epoch 99/2000\n",
            "816/816 [==============================] - 0s 532us/step - loss: 0.0322 - val_loss: 0.0291\n",
            "Epoch 100/2000\n",
            "816/816 [==============================] - 0s 567us/step - loss: 0.0345 - val_loss: 0.0322\n",
            "Epoch 101/2000\n",
            "816/816 [==============================] - 0s 530us/step - loss: 0.0335 - val_loss: 0.0275\n",
            "Epoch 102/2000\n",
            "816/816 [==============================] - 0s 561us/step - loss: 0.0336 - val_loss: 0.0290\n",
            "Epoch 103/2000\n",
            "816/816 [==============================] - 0s 531us/step - loss: 0.0334 - val_loss: 0.0275\n",
            "Epoch 104/2000\n",
            "816/816 [==============================] - 0s 519us/step - loss: 0.0320 - val_loss: 0.0303\n",
            "Epoch 105/2000\n",
            "816/816 [==============================] - 0s 551us/step - loss: 0.0339 - val_loss: 0.0273\n",
            "Epoch 106/2000\n",
            "816/816 [==============================] - 0s 515us/step - loss: 0.0332 - val_loss: 0.0327\n",
            "Epoch 107/2000\n",
            "816/816 [==============================] - 0s 544us/step - loss: 0.0346 - val_loss: 0.0297\n",
            "Epoch 108/2000\n",
            "816/816 [==============================] - 0s 549us/step - loss: 0.0327 - val_loss: 0.0272\n",
            "Epoch 109/2000\n",
            "816/816 [==============================] - 0s 556us/step - loss: 0.0373 - val_loss: 0.0289\n",
            "Epoch 110/2000\n",
            "816/816 [==============================] - 0s 528us/step - loss: 0.0336 - val_loss: 0.0291\n",
            "Epoch 111/2000\n",
            "816/816 [==============================] - 0s 539us/step - loss: 0.0349 - val_loss: 0.0309\n",
            "Epoch 112/2000\n",
            "816/816 [==============================] - 0s 541us/step - loss: 0.0331 - val_loss: 0.0281\n",
            "Epoch 113/2000\n",
            "816/816 [==============================] - 0s 538us/step - loss: 0.0337 - val_loss: 0.0304\n",
            "Epoch 114/2000\n",
            "816/816 [==============================] - 0s 528us/step - loss: 0.0330 - val_loss: 0.0266\n",
            "Epoch 115/2000\n",
            "816/816 [==============================] - 0s 528us/step - loss: 0.0337 - val_loss: 0.0359\n",
            "Epoch 116/2000\n",
            "816/816 [==============================] - 0s 536us/step - loss: 0.0345 - val_loss: 0.0296\n",
            "Epoch 117/2000\n",
            "816/816 [==============================] - 0s 529us/step - loss: 0.0332 - val_loss: 0.0293\n",
            "Epoch 118/2000\n",
            "816/816 [==============================] - 0s 537us/step - loss: 0.0323 - val_loss: 0.0292\n",
            "Epoch 119/2000\n",
            "816/816 [==============================] - 0s 550us/step - loss: 0.0330 - val_loss: 0.0329\n",
            "Epoch 120/2000\n",
            "816/816 [==============================] - 0s 533us/step - loss: 0.0371 - val_loss: 0.0269\n",
            "Epoch 121/2000\n",
            "816/816 [==============================] - 0s 551us/step - loss: 0.0336 - val_loss: 0.0285\n",
            "Epoch 122/2000\n",
            "816/816 [==============================] - 0s 535us/step - loss: 0.0351 - val_loss: 0.0270\n",
            "Epoch 123/2000\n",
            "816/816 [==============================] - 0s 572us/step - loss: 0.0319 - val_loss: 0.0280\n",
            "Epoch 124/2000\n",
            "816/816 [==============================] - 0s 539us/step - loss: 0.0323 - val_loss: 0.0298\n",
            "Epoch 125/2000\n",
            "816/816 [==============================] - 0s 540us/step - loss: 0.0319 - val_loss: 0.0281\n",
            "Epoch 126/2000\n",
            "816/816 [==============================] - 0s 562us/step - loss: 0.0334 - val_loss: 0.0303\n",
            "Epoch 127/2000\n",
            "816/816 [==============================] - 0s 543us/step - loss: 0.0362 - val_loss: 0.0346\n",
            "Epoch 128/2000\n",
            "816/816 [==============================] - 0s 539us/step - loss: 0.0399 - val_loss: 0.0382\n",
            "Epoch 129/2000\n",
            "816/816 [==============================] - 0s 537us/step - loss: 0.0399 - val_loss: 0.0299\n",
            "Epoch 130/2000\n",
            "816/816 [==============================] - 0s 542us/step - loss: 0.0388 - val_loss: 0.0412\n",
            "Epoch 131/2000\n",
            "816/816 [==============================] - 0s 536us/step - loss: 0.0361 - val_loss: 0.0274\n",
            "Epoch 132/2000\n",
            "816/816 [==============================] - 0s 553us/step - loss: 0.0325 - val_loss: 0.0302\n",
            "Epoch 133/2000\n",
            "816/816 [==============================] - 0s 534us/step - loss: 0.0341 - val_loss: 0.0279\n",
            "Epoch 134/2000\n",
            "816/816 [==============================] - 0s 552us/step - loss: 0.0362 - val_loss: 0.0335\n",
            "Epoch 135/2000\n",
            "816/816 [==============================] - 0s 543us/step - loss: 0.0361 - val_loss: 0.0306\n",
            "Epoch 136/2000\n",
            "816/816 [==============================] - 0s 553us/step - loss: 0.0340 - val_loss: 0.0270\n",
            "Epoch 137/2000\n",
            "816/816 [==============================] - 0s 543us/step - loss: 0.0338 - val_loss: 0.0318\n",
            "Epoch 138/2000\n",
            "816/816 [==============================] - 0s 531us/step - loss: 0.0334 - val_loss: 0.0253\n",
            "Epoch 139/2000\n",
            "816/816 [==============================] - 0s 541us/step - loss: 0.0318 - val_loss: 0.0254\n",
            "Epoch 140/2000\n",
            "816/816 [==============================] - 0s 534us/step - loss: 0.0318 - val_loss: 0.0266\n",
            "Epoch 141/2000\n",
            "816/816 [==============================] - 0s 554us/step - loss: 0.0312 - val_loss: 0.0261\n",
            "Epoch 142/2000\n",
            "816/816 [==============================] - 0s 546us/step - loss: 0.0308 - val_loss: 0.0318\n",
            "Epoch 143/2000\n",
            "816/816 [==============================] - 0s 539us/step - loss: 0.0345 - val_loss: 0.0268\n",
            "Epoch 144/2000\n",
            "816/816 [==============================] - 0s 557us/step - loss: 0.0326 - val_loss: 0.0273\n",
            "Epoch 145/2000\n",
            "816/816 [==============================] - 0s 562us/step - loss: 0.0309 - val_loss: 0.0270\n",
            "Epoch 146/2000\n",
            "816/816 [==============================] - 0s 540us/step - loss: 0.0303 - val_loss: 0.0251\n",
            "Epoch 147/2000\n",
            "816/816 [==============================] - 0s 552us/step - loss: 0.0311 - val_loss: 0.0254\n",
            "Epoch 148/2000\n",
            "816/816 [==============================] - 0s 553us/step - loss: 0.0307 - val_loss: 0.0300\n",
            "Epoch 149/2000\n",
            "816/816 [==============================] - 0s 549us/step - loss: 0.0328 - val_loss: 0.0293\n",
            "Epoch 150/2000\n",
            "816/816 [==============================] - 0s 544us/step - loss: 0.0322 - val_loss: 0.0292\n",
            "Epoch 151/2000\n",
            "816/816 [==============================] - 0s 541us/step - loss: 0.0348 - val_loss: 0.0280\n",
            "Epoch 152/2000\n",
            "816/816 [==============================] - 0s 553us/step - loss: 0.0325 - val_loss: 0.0364\n",
            "Epoch 153/2000\n",
            "816/816 [==============================] - 0s 538us/step - loss: 0.0350 - val_loss: 0.0298\n",
            "Epoch 154/2000\n",
            "816/816 [==============================] - 0s 554us/step - loss: 0.0311 - val_loss: 0.0276\n",
            "Epoch 155/2000\n",
            "816/816 [==============================] - 0s 555us/step - loss: 0.0355 - val_loss: 0.0266\n",
            "Epoch 156/2000\n",
            "816/816 [==============================] - 0s 557us/step - loss: 0.0365 - val_loss: 0.0283\n",
            "Epoch 157/2000\n",
            "816/816 [==============================] - 0s 539us/step - loss: 0.0320 - val_loss: 0.0259\n",
            "Epoch 158/2000\n",
            "816/816 [==============================] - 0s 543us/step - loss: 0.0311 - val_loss: 0.0265\n",
            "Epoch 159/2000\n",
            "816/816 [==============================] - 0s 543us/step - loss: 0.0331 - val_loss: 0.0294\n",
            "Epoch 160/2000\n",
            "816/816 [==============================] - 0s 561us/step - loss: 0.0332 - val_loss: 0.0280\n",
            "Epoch 161/2000\n",
            "816/816 [==============================] - 0s 549us/step - loss: 0.0297 - val_loss: 0.0267\n",
            "Epoch 162/2000\n",
            "816/816 [==============================] - 0s 542us/step - loss: 0.0304 - val_loss: 0.0284\n",
            "Epoch 163/2000\n",
            "816/816 [==============================] - 0s 539us/step - loss: 0.0301 - val_loss: 0.0269\n",
            "Epoch 164/2000\n",
            "816/816 [==============================] - 0s 537us/step - loss: 0.0323 - val_loss: 0.0303\n",
            "Epoch 165/2000\n",
            "816/816 [==============================] - 0s 547us/step - loss: 0.0340 - val_loss: 0.0339\n",
            "Epoch 166/2000\n",
            "816/816 [==============================] - 0s 531us/step - loss: 0.0312 - val_loss: 0.0260\n",
            "Epoch 167/2000\n",
            "816/816 [==============================] - 0s 546us/step - loss: 0.0310 - val_loss: 0.0274\n",
            "Epoch 168/2000\n",
            "816/816 [==============================] - 0s 545us/step - loss: 0.0305 - val_loss: 0.0249\n",
            "Epoch 169/2000\n",
            "816/816 [==============================] - 0s 553us/step - loss: 0.0306 - val_loss: 0.0259\n",
            "Epoch 170/2000\n",
            "816/816 [==============================] - 0s 536us/step - loss: 0.0309 - val_loss: 0.0248\n",
            "Epoch 171/2000\n",
            "816/816 [==============================] - 0s 533us/step - loss: 0.0316 - val_loss: 0.0254\n",
            "Epoch 172/2000\n",
            "816/816 [==============================] - 0s 538us/step - loss: 0.0312 - val_loss: 0.0263\n",
            "Epoch 173/2000\n",
            "816/816 [==============================] - 0s 538us/step - loss: 0.0317 - val_loss: 0.0296\n",
            "Epoch 174/2000\n",
            "816/816 [==============================] - 0s 538us/step - loss: 0.0327 - val_loss: 0.0308\n",
            "Epoch 175/2000\n",
            "816/816 [==============================] - 0s 529us/step - loss: 0.0327 - val_loss: 0.0258\n",
            "Epoch 176/2000\n",
            "816/816 [==============================] - 0s 536us/step - loss: 0.0318 - val_loss: 0.0254\n",
            "Epoch 177/2000\n",
            "816/816 [==============================] - 0s 553us/step - loss: 0.0301 - val_loss: 0.0304\n",
            "Epoch 178/2000\n",
            "816/816 [==============================] - 0s 546us/step - loss: 0.0310 - val_loss: 0.0277\n",
            "Epoch 179/2000\n",
            "816/816 [==============================] - 0s 560us/step - loss: 0.0302 - val_loss: 0.0273\n",
            "Epoch 180/2000\n",
            "816/816 [==============================] - 0s 563us/step - loss: 0.0302 - val_loss: 0.0268\n",
            "Epoch 181/2000\n",
            "816/816 [==============================] - 0s 550us/step - loss: 0.0314 - val_loss: 0.0271\n",
            "Epoch 182/2000\n",
            "816/816 [==============================] - 0s 552us/step - loss: 0.0336 - val_loss: 0.0261\n",
            "Epoch 183/2000\n",
            "816/816 [==============================] - 0s 551us/step - loss: 0.0328 - val_loss: 0.0277\n",
            "Epoch 184/2000\n",
            "816/816 [==============================] - 0s 570us/step - loss: 0.0311 - val_loss: 0.0272\n",
            "Epoch 185/2000\n",
            "816/816 [==============================] - 0s 559us/step - loss: 0.0303 - val_loss: 0.0277\n",
            "Epoch 186/2000\n",
            "816/816 [==============================] - 0s 552us/step - loss: 0.0308 - val_loss: 0.0268\n",
            "Epoch 187/2000\n",
            "816/816 [==============================] - 0s 561us/step - loss: 0.0302 - val_loss: 0.0247\n",
            "Epoch 188/2000\n",
            "816/816 [==============================] - 0s 550us/step - loss: 0.0294 - val_loss: 0.0252\n",
            "Epoch 189/2000\n",
            "816/816 [==============================] - 0s 546us/step - loss: 0.0296 - val_loss: 0.0252\n",
            "Epoch 190/2000\n",
            "816/816 [==============================] - 0s 565us/step - loss: 0.0298 - val_loss: 0.0247\n",
            "Epoch 191/2000\n",
            "816/816 [==============================] - 0s 561us/step - loss: 0.0302 - val_loss: 0.0270\n",
            "Epoch 192/2000\n",
            "816/816 [==============================] - 0s 550us/step - loss: 0.0291 - val_loss: 0.0237\n",
            "Epoch 193/2000\n",
            "816/816 [==============================] - 0s 550us/step - loss: 0.0316 - val_loss: 0.0293\n",
            "Epoch 194/2000\n",
            "816/816 [==============================] - 0s 558us/step - loss: 0.0313 - val_loss: 0.0252\n",
            "Epoch 195/2000\n",
            "816/816 [==============================] - 0s 562us/step - loss: 0.0303 - val_loss: 0.0251\n",
            "Epoch 196/2000\n",
            "816/816 [==============================] - 0s 564us/step - loss: 0.0304 - val_loss: 0.0291\n",
            "Epoch 197/2000\n",
            "816/816 [==============================] - 0s 551us/step - loss: 0.0302 - val_loss: 0.0255\n",
            "Epoch 198/2000\n",
            "816/816 [==============================] - 0s 549us/step - loss: 0.0296 - val_loss: 0.0297\n",
            "Epoch 199/2000\n",
            "816/816 [==============================] - 0s 566us/step - loss: 0.0308 - val_loss: 0.0282\n",
            "Epoch 200/2000\n",
            "816/816 [==============================] - 0s 554us/step - loss: 0.0326 - val_loss: 0.0343\n",
            "Epoch 201/2000\n",
            "816/816 [==============================] - 0s 562us/step - loss: 0.0341 - val_loss: 0.0278\n",
            "Epoch 202/2000\n",
            "816/816 [==============================] - 0s 562us/step - loss: 0.0315 - val_loss: 0.0263\n",
            "Epoch 203/2000\n",
            "816/816 [==============================] - 0s 550us/step - loss: 0.0321 - val_loss: 0.0324\n",
            "Epoch 204/2000\n",
            "816/816 [==============================] - 0s 543us/step - loss: 0.0341 - val_loss: 0.0282\n",
            "Epoch 205/2000\n",
            "816/816 [==============================] - 0s 565us/step - loss: 0.0315 - val_loss: 0.0288\n",
            "Epoch 206/2000\n",
            "816/816 [==============================] - 0s 575us/step - loss: 0.0320 - val_loss: 0.0273\n",
            "Epoch 207/2000\n",
            "816/816 [==============================] - 0s 559us/step - loss: 0.0324 - val_loss: 0.0335\n",
            "Epoch 208/2000\n",
            "816/816 [==============================] - 0s 561us/step - loss: 0.0329 - val_loss: 0.0273\n",
            "Epoch 209/2000\n",
            "816/816 [==============================] - 0s 553us/step - loss: 0.0310 - val_loss: 0.0269\n",
            "Epoch 210/2000\n",
            "816/816 [==============================] - 0s 564us/step - loss: 0.0299 - val_loss: 0.0275\n",
            "Epoch 211/2000\n",
            "816/816 [==============================] - 0s 562us/step - loss: 0.0292 - val_loss: 0.0261\n",
            "Epoch 212/2000\n",
            "816/816 [==============================] - 0s 559us/step - loss: 0.0323 - val_loss: 0.0282\n",
            "Epoch 213/2000\n",
            "816/816 [==============================] - 0s 562us/step - loss: 0.0329 - val_loss: 0.0260\n",
            "Epoch 214/2000\n",
            "816/816 [==============================] - 0s 561us/step - loss: 0.0313 - val_loss: 0.0269\n",
            "Epoch 215/2000\n",
            "816/816 [==============================] - 0s 544us/step - loss: 0.0332 - val_loss: 0.0314\n",
            "Epoch 216/2000\n",
            "816/816 [==============================] - 0s 570us/step - loss: 0.0368 - val_loss: 0.0311\n",
            "Epoch 217/2000\n",
            "816/816 [==============================] - 0s 552us/step - loss: 0.0338 - val_loss: 0.0279\n",
            "Epoch 218/2000\n",
            "816/816 [==============================] - 0s 552us/step - loss: 0.0308 - val_loss: 0.0275\n",
            "Epoch 219/2000\n",
            "816/816 [==============================] - 0s 561us/step - loss: 0.0303 - val_loss: 0.0288\n",
            "Epoch 220/2000\n",
            "816/816 [==============================] - 0s 549us/step - loss: 0.0315 - val_loss: 0.0242\n",
            "Epoch 221/2000\n",
            "816/816 [==============================] - 0s 566us/step - loss: 0.0284 - val_loss: 0.0267\n",
            "Epoch 222/2000\n",
            "816/816 [==============================] - 0s 547us/step - loss: 0.0307 - val_loss: 0.0253\n",
            "Epoch 223/2000\n",
            "816/816 [==============================] - 0s 555us/step - loss: 0.0352 - val_loss: 0.0294\n",
            "Epoch 224/2000\n",
            "816/816 [==============================] - 0s 544us/step - loss: 0.0352 - val_loss: 0.0365\n",
            "Epoch 225/2000\n",
            "816/816 [==============================] - 0s 552us/step - loss: 0.0328 - val_loss: 0.0274\n",
            "Epoch 226/2000\n",
            "816/816 [==============================] - 0s 560us/step - loss: 0.0297 - val_loss: 0.0257\n",
            "Epoch 227/2000\n",
            "816/816 [==============================] - 0s 553us/step - loss: 0.0302 - val_loss: 0.0295\n",
            "Epoch 228/2000\n",
            "816/816 [==============================] - 0s 559us/step - loss: 0.0328 - val_loss: 0.0261\n",
            "Epoch 229/2000\n",
            "816/816 [==============================] - 0s 548us/step - loss: 0.0305 - val_loss: 0.0252\n",
            "Epoch 230/2000\n",
            "816/816 [==============================] - 0s 559us/step - loss: 0.0284 - val_loss: 0.0257\n",
            "Epoch 231/2000\n",
            "816/816 [==============================] - 0s 540us/step - loss: 0.0295 - val_loss: 0.0272\n",
            "Epoch 232/2000\n",
            "816/816 [==============================] - 0s 557us/step - loss: 0.0294 - val_loss: 0.0255\n",
            "Epoch 233/2000\n",
            "816/816 [==============================] - 0s 562us/step - loss: 0.0304 - val_loss: 0.0266\n",
            "Epoch 234/2000\n",
            "816/816 [==============================] - 0s 570us/step - loss: 0.0298 - val_loss: 0.0244\n",
            "Epoch 235/2000\n",
            "816/816 [==============================] - 0s 558us/step - loss: 0.0311 - val_loss: 0.0250\n",
            "Epoch 236/2000\n",
            "816/816 [==============================] - 0s 578us/step - loss: 0.0296 - val_loss: 0.0253\n",
            "Epoch 237/2000\n",
            "816/816 [==============================] - 0s 596us/step - loss: 0.0291 - val_loss: 0.0257\n",
            "Epoch 238/2000\n",
            "816/816 [==============================] - 0s 573us/step - loss: 0.0289 - val_loss: 0.0245\n",
            "Epoch 239/2000\n",
            "816/816 [==============================] - 0s 546us/step - loss: 0.0295 - val_loss: 0.0260\n",
            "Epoch 240/2000\n",
            "816/816 [==============================] - 0s 549us/step - loss: 0.0307 - val_loss: 0.0305\n",
            "Epoch 241/2000\n",
            "816/816 [==============================] - 0s 571us/step - loss: 0.0325 - val_loss: 0.0279\n",
            "Epoch 242/2000\n",
            "816/816 [==============================] - 0s 552us/step - loss: 0.0295 - val_loss: 0.0250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGGx-WpzTLCM",
        "colab_type": "code",
        "outputId": "a4661aaa-527c-4485-baf6-ac5392219b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Val_loss valdation\n",
        "print(np.asarray(storia.history[\"val_loss\"]).min())\n",
        "\n",
        "#Val_loss test\n",
        "estimation = model.predict(x_test)\n",
        "evaluate_predictions(y_test, estimation)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.023658338943313734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03249989385580286"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}